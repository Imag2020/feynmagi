













Search | arXiv e-print repository












Skip to main content





We gratefully acknowledge support from the Simons Foundation and member institutions.















Help | Advanced Search




All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text




Search



 


Login







    
        Showing 1–50 of 15,114 results for all: LLM




Search v0.5.6 released 2020-02-24  
Feedback?






Search term or terms



Field
All fieldsTitleAuthor(s)AbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDLicense (URI)arXiv author IDHelp pagesFull text


Search





 Show abstracts
        

 Hide abstracts
        




Advanced Search









All fieldsTitleAuthor(s)AbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDLicense (URI)arXiv author IDHelp pagesFull text

 Show abstracts Hide abstracts




2550100200

results per page.
        

Sort results by

Announcement date (newest first)Announcement date (oldest first)Submission date (newest first)Submission date (oldest first)Relevance



Go






Previous
    
Next
      


1
        


2
            


3
            


4
            


5
            

…





arXiv:2407.21018
 [pdf, other] 


cs.CL
cs.AI



      
        ThinK: Thinner Key Cache by Query-Driven Pruning
      
    

Authors:
Yuhui Xu, 
      
      Zhanming Jie, 
      
      Hanze Dong, 
      
      Lei Wang, 
      
      Xudong Lu, 
      
      Aojun Zhou, 
      
      Amrita Saha, 
      
      Caiming Xiong, 
      
      Doyen Sahoo


Abstract:
      
        Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications by leveraging increased model sizes and sequence lengths. However, the associated rise in computational and memory costs poses significant challenges, particularly in managing long sequences due to th…
        ▽ More


        Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications by leveraging increased model sizes and sequence lengths. However, the associated rise in computational and memory costs poses significant challenges, particularly in managing long sequences due to the quadratic complexity of the transformer attention mechanism. This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. Unlike existing approaches that optimize the memory based on the sequence lengths, we uncover that the channel dimension of the KV cache exhibits significant redundancy, characterized by unbalanced magnitude distribution and low-rank structure in attention weights. Based on these observations, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in memory costs by over 20% compared with vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and Mistral models across various long-sequence datasets confirm the efficacy of ThinK, setting a new precedent for efficient LLM deployment without compromising performance. We also outline the potential of extending our method to value cache pruning, demonstrating ThinK's versatility and broad applicability in reducing both memory and computational overheads.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    

Comments:
20 pages, 6 figures




arXiv:2407.21009
 [pdf, other] 


cs.AI



      
        AI-Assisted Generation of Difficult Math Questions
      
    

Authors:
Vedant Shah, 
      
      Dingli Yu, 
      
      Kaifeng Lyu, 
      
      Simon Park, 
      
      Nan Rosemary Ke, 
      
      Michael Mozer, 
      
      Yoshua Bengio, 
      
      Sanjeev Arora, 
      
      Anirudh Goyal


Abstract:
      
        Current LLM training positions mathematical reasoning as a core capability. With publicly available sources fully tapped, there is unmet demand for diverse and challenging math questions. Relying solely on human experts is both time-consuming and costly, while…
        ▽ More


        Current LLM training positions mathematical reasoning as a core capability. With publicly available sources fully tapped, there is unmet demand for diverse and challenging math questions. Relying solely on human experts is both time-consuming and costly, while LLM-generated questions often lack the requisite diversity and difficulty. We present a design framework that combines the strengths of LLMs with a human-in-the-loop approach to generate a diverse array of challenging math questions. We leverage LLM metacognition skills [Didolkar et al., 2024] of a strong LLM to extract core "skills" from existing math datasets. These skills serve as the basis for generating novel and difficult questions by prompting the LLM with random pairs of core skills. The use of two different skills within each question makes finding such questions an "out of distribution" task for both LLMs and humans. Our pipeline employs LLMs to iteratively generate and refine questions and solutions through multiturn prompting. Human annotators then verify and further refine the questions, with their efficiency enhanced via further LLM interactions. Applying this pipeline on skills extracted from the MATH dataset [Hendrycks et al., 2021] resulted in MATH2 - a dataset of higher-quality math questions, as evidenced by: (a) Lower performance of all models on MATH2 than on MATH (b) Higher performance on MATH when using MATH2 questions as in-context examples. Although focused on mathematics, our methodology seems applicable to other domains requiring structured reasoning, and potentially as a component of scalable oversight. Also of interest is a striking relationship observed between models' performance on the new dataset: the success rate on MATH2 is the square on MATH, suggesting that successfully solving the question in MATH2 requires a nontrivial combination of two distinct math skills.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    



arXiv:2407.20999
 [pdf, other] 


cs.LG
cs.AI



      
        MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning
      
    

Authors:
Yupeng Chen, 
      
      Senmiao Wang, 
      
      Zhihang Lin, 
      
      Zeyu Qin, 
      
      Yushun Zhang, 
      
      Tian Ding, 
      
      Ruoyu Sun


Abstract:
      
        Recently, large language models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks. Typically, an…
        ▽ More


        Recently, large language models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks. Typically, an LLM is pre-trained on large corpora and subsequently fine-tuned on task-specific datasets. However, during finetuning, LLMs may forget the knowledge acquired in the pretraining stage, leading to a decline in general capabilities. To address this issue, we propose a new fine-tuning algorithm termed Momentum-Filtered Optimizer (MoFO). The key idea of MoFO is to iteratively select and update the model parameters with the largest momentum magnitudes. Compared to full-parameter training, MoFO achieves similar fine-tuning performance while keeping parameters closer to the pre-trained model, thereby mitigating knowledge forgetting. Unlike most existing methods for forgetting mitigation, MoFO combines the following two advantages. First, MoFO does not require access to pre-training data. This makes MoFO particularly suitable for fine-tuning scenarios where pre-training data is unavailable, such as fine-tuning checkpoint-only open-source LLMs. Second, MoFO does not alter the original loss function. This could avoid impairing the model performance on the fine-tuning tasks. We validate MoFO through rigorous convergence analysis and extensive experiments, demonstrating its superiority over existing methods in mitigating forgetting and enhancing fine-tuning performance.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    



arXiv:2407.20990
 [pdf, other] 


cs.AI
cs.CL
cs.CV
cs.HC
cs.LG



      
        From Feature Importance to Natural Language Explanations Using LLMs with RAG
      
    

Authors:
Sule Tekkesinoglu, 
      
      Lars Kunze


Abstract:
      
        …of predictive models. In this work, we introduce traceable question-answering, leveraging an external knowledge repository to inform the responses of Large Language Models (LLMs) to user queries within a scene understanding task. This knowledge repository comprises contextual details regarding the model's output, containing high-level features, feature i…
        ▽ More


        As machine learning becomes increasingly integral to autonomous decision-making processes involving human interaction, the necessity of comprehending the model's outputs through conversational means increases. Most recently, foundation models are being explored for their potential as post hoc explainers, providing a pathway to elucidate the decision-making mechanisms of predictive models. In this work, we introduce traceable question-answering, leveraging an external knowledge repository to inform the responses of Large Language Models (LLMs) to user queries within a scene understanding task. This knowledge repository comprises contextual details regarding the model's output, containing high-level features, feature importance, and alternative probabilities. We employ subtractive counterfactual reasoning to compute feature importance, a method that entails analysing output variations resulting from decomposing semantic features. Furthermore, to maintain a seamless conversational flow, we integrate four key characteristics - social, causal, selective, and contrastive - drawn from social science research on human explanations into a single-shot prompt, guiding the response generation process. Our evaluation demonstrates that explanations generated by the LLMs encompassed these elements, indicating its potential to bridge the gap between complex model outputs and natural language expressions.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    



arXiv:2407.20970
 [pdf, other] 


cs.NI
cs.AI



      
        Large Language Models (LLMs) for Semantic Communication in Edge-based IoT Networks
      
    

Authors:
Alakesh Kalita


Abstract:
      
        …communication is gaining attention among researchers as current communication technologies are approaching Shannon's limit. On the other hand, Large Language Models (LLMs) can understand and generate human-like text, based on extensive training on diverse datasets with billions of parameters. Considering the recent near-source computational technologies…
        ▽ More


        With the advent of Fifth Generation (5G) and Sixth Generation (6G) communication technologies, as well as the Internet of Things (IoT), semantic communication is gaining attention among researchers as current communication technologies are approaching Shannon's limit. On the other hand, Large Language Models (LLMs) can understand and generate human-like text, based on extensive training on diverse datasets with billions of parameters. Considering the recent near-source computational technologies like Edge, in this article, we give an overview of a framework along with its modules, where LLMs can be used under the umbrella of semantic communication at the network edge for efficient communication in IoT networks. Finally, we discuss a few applications and analyze the challenges and opportunities to develop such systems.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    

Comments:
6pages, 3 figures, Magazine




arXiv:2407.20962
 [pdf, other] 


cs.CV
cs.MM
cs.SD
eess.AS



      
        MMTrail: A Multimodal Trailer Video Dataset with Language and Music Descriptions
      
    

Authors:
Xiaowei Chi, 
      
      Yatian Wang, 
      
      Aosong Cheng, 
      
      Pengjun Fang, 
      
      Zeyue Tian, 
      
      Yingqing He, 
      
      Zhaoyang Liu, 
      
      Xingqun Qi, 
      
      Jiahao Pan, 
      
      Rongyu Zhang, 
      
      Mengfei Li, 
      
      Ruibin Yuan, 
      
      Yanbing Jiang, 
      
      Wei Xue, 
      
      Wenhan Luo, 
      
      Qifeng Chen, 
      
      Shanghang Zhang, 
      
      Qifeng Liu, 
      
      Yike Guo


Abstract:
      
        …with more than 27.1k hours of trailer videos. Here, to ensure the caption retains music perspective while preserving the authority of visual context, we leverage the advanced LLM to merge all annotations adaptively. In this fashion, our MMtrail dataset potentially paves the path for fine-grained large multimodal-language model training. In experiments, we pr…
        ▽ More


        Massive multi-modality datasets play a significant role in facilitating the success of large video-language models. However, current video-language datasets primarily provide text descriptions for visual frames, considering audio to be weakly related information. They usually overlook exploring the potential of inherent audio-visual correlation, leading to monotonous annotation within each modality instead of comprehensive and precise descriptions. Such ignorance results in the difficulty of multiple cross-modality studies. To fulfill this gap, we present MMTrail, a large-scale multi-modality video-language dataset incorporating more than 20M trailer clips with visual captions, and 2M high-quality clips with multimodal captions. Trailers preview full-length video works and integrate context, visual frames, and background music. In particular, the trailer has two main advantages: (1) the topics are diverse, and the content characters are of various types, e.g., film, news, and gaming. (2) the corresponding background music is custom-designed, making it more coherent with the visual context. Upon these insights, we propose a systemic captioning framework, achieving various modality annotations with more than 27.1k hours of trailer videos. Here, to ensure the caption retains music perspective while preserving the authority of visual context, we leverage the advanced LLM to merge all annotations adaptively. In this fashion, our MMtrail dataset potentially paves the path for fine-grained large multimodal-language model training. In experiments, we provide evaluation metrics and benchmark results on our dataset, demonstrating the high quality of our annotation and its effectiveness for model training.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    

Comments:
15 Pages. Dataset report




arXiv:2407.20920
 [pdf, other] 


cs.CV



      
        SSPA: Split-and-Synthesize Prompting with Gated Alignments for Multi-Label Image Recognition
      
    

Authors:
Hao Tan, 
      
      Zichang Tan, 
      
      Jun Li, 
      
      Jun Wan, 
      
      Zhen Lei, 
      
      Stan Z. Li


Abstract:
      
        …with Gated Alignments (SSPA) framework to amplify the potential of VLMs. Specifically, we develop an in-context learning approach to associate the inherent knowledge from LLMs. Then we propose a novel Split-and-Synthesize Prompting (SSP) strategy to first model the generic knowledge and downstream label semantics individually and then aggregate them carefull…
        ▽ More


        Multi-label image recognition is a fundamental task in computer vision. Recently, Vision-Language Models (VLMs) have made notable advancements in this area. However, previous methods fail to effectively leverage the rich knowledge in language models and often incorporate label semantics into visual features unidirectionally. To overcome these problems, we propose a Split-and-Synthesize Prompting with Gated Alignments (SSPA) framework to amplify the potential of VLMs. Specifically, we develop an in-context learning approach to associate the inherent knowledge from LLMs. Then we propose a novel Split-and-Synthesize Prompting (SSP) strategy to first model the generic knowledge and downstream label semantics individually and then aggregate them carefully through the quaternion network. Moreover, we present Gated Dual-Modal Alignments (GDMA) to bidirectionally interact visual and linguistic modalities while eliminating redundant cross-modal information, enabling more efficient region-level alignments. Rather than making the final prediction by a sharp manner in previous works, we propose a soft aggregator to jointly consider results from all image regions. With the help of flexible prompting and gated alignments, SSPA is generalizable to specific domains. Extensive experiments on nine datasets from three domains (i.e., natural, pedestrian attributes and remote sensing) demonstrate the state-of-the-art performance of SSPA. Further analyses verify the effectiveness of SSP and the interpretability of GDMA. The code will be made public.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    

Comments:
13 pages, 8 figures




arXiv:2407.20906
 [pdf, other] 


cs.CL
cs.AI
physics.data-an



      
        Automated Review Generation Method Based on Large Language Models
      
    

Authors:
Shican Wu, 
      
      Xiao Ma, 
      
      Dehui Luo, 
      
      Lulu Li, 
      
      Xiangcheng Shi, 
      
      Xin Chang, 
      
      Xiaoyun Lin, 
      
      Ran Luo, 
      
      Chunlei Pei, 
      
      Zhi-Jian Zhao, 
      
      Jinlong Gong


Abstract:
      
        …advancement, is overwhelmed by the vast ocean of available information. Addressing this, we propose an automated review generation method based on Large Language Models (LLMs) to streamline literature processing and reduce cognitive load. In case study on propane dehydrogenation (PDH) catalysts, our method swiftly generated comprehensive reviews from 343 art…
        ▽ More


        Literature research, vital for scientific advancement, is overwhelmed by the vast ocean of available information. Addressing this, we propose an automated review generation method based on Large Language Models (LLMs) to streamline literature processing and reduce cognitive load. In case study on propane dehydrogenation (PDH) catalysts, our method swiftly generated comprehensive reviews from 343 articles, averaging seconds per article per LLM account. Extended analysis of 1041 articles provided deep insights into catalysts' composition, structure, and performance. Recognizing LLMs' hallucinations, we employed a multi-layered quality control strategy, ensuring our method's reliability and effective hallucination mitigation. Expert verification confirms the accuracy and citation integrity of generated reviews, demonstrating LLM hallucination risks reduced to below 0.5% with over 95% confidence. Released Windows application enables one-click review generation, aiding researchers in tracking advancements and recommending literature. This approach showcases LLMs' role in enhancing scientific research productivity and sets the stage for further exploration.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    

Comments:
16 pages, 3 figures, 3 tables




arXiv:2407.20898
 [pdf, other] 


cs.SE



      
        ThinkRepair: Self-Directed Automated Program Repair
      
    

Authors:
Xin Yin, 
      
      Chao Ni, 
      
      Shaohua Wang, 
      
      Zhenhao Li, 
      
      Limin Zeng, 
      
      Xiaohu Yang


Abstract:
      
        …remarkable performance, they still have limitations in fixing bugs that require analyzing and reasoning about the logic of the buggy program. Recently, large language models (LLMs) instructed by prompt engineering have attracted much attention for their powerful ability to address many kinds of tasks including bug-fixing. However, the quality of the prompt w…
        ▽ More


        Though many approaches have been proposed for Automated Program Repair (APR) and indeed achieved remarkable performance, they still have limitations in fixing bugs that require analyzing and reasoning about the logic of the buggy program. Recently, large language models (LLMs) instructed by prompt engineering have attracted much attention for their powerful ability to address many kinds of tasks including bug-fixing. However, the quality of the prompt will highly affect the ability of LLMs and manually constructing high-quality prompts is a costly endeavor.
  To address this limitation, we propose a self-directed LLM-based automated program repair, ThinkRepair, with two main phases: collection phase and fixing phase. The former phase automatically collects various chains of thoughts that constitute pre-fixed knowledge by instructing LLMs with the Chain-of-Thought (CoT) prompt. The latter phase targets fixing a bug by first selecting examples for few-shot learning and second automatically interacting with LLMs, optionally appending with feedback of testing information.
  Evaluations on two widely studied datasets (Defects4J and QuixBugs) by comparing ThinkRepair with 12 SOTA APRs indicate the priority of ThinkRepair in fixing bugs. Notably, ThinkRepair fixes 98 bugs and improves baselines by 27%-344.4% on Defects4J V1.2. On Defects4J V2.0, ThinkRepair fixes 12-65 more bugs than the SOTA APRs. Additionally, ThinkRepair also makes a considerable improvement on QuixBugs (31 for Java and 21 for Python at most).
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    

Comments:
Accepted By ISSTA'24




arXiv:2407.20859
 [pdf, other] 


cs.CR
cs.LG



      
        Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification
      
    

Authors:
Boyang Zhang, 
      
      Yicong Tan, 
      
      Yun Shen, 
      
      Ahmed Salem, 
      
      Michael Backes, 
      
      Savvas Zannettou, 
      
      Yang Zhang


Abstract:
      
        Recently, autonomous agents built on large language models (LLMs) have experienced significant development and are being deployed in real-world applications. These agents can extend the base…
        ▽ More


        Recently, autonomous agents built on large language models (LLMs) have experienced significant development and are being deployed in real-world applications. These agents can extend the base LLM's capabilities in multiple ways. For example, a well-built agent using GPT-3.5-Turbo as its core can outperform the more advanced GPT-4 model by leveraging external components. More importantly, the usage of tools enables these systems to perform actions in the real world, moving from merely generating text to actively interacting with their environment. Given the agents' practical applications and their ability to execute consequential actions, it is crucial to assess potential vulnerabilities. Such autonomous systems can cause more severe damage than a standalone language model if compromised. While some existing research has explored harmful actions by LLM agents, our study approaches the vulnerability from a different perspective. We introduce a new type of attack that causes malfunctions by misleading the agent into executing repetitive or irrelevant actions. We conduct comprehensive evaluations using various attack methods, surfaces, and properties to pinpoint areas of susceptibility. Our experiments reveal that these attacks can induce failure rates exceeding 80\% in multiple scenarios. Through attacks on implemented and deployable agents in multi-agent scenarios, we accentuate the realistic risks associated with these vulnerabilities. To mitigate such attacks, we propose self-examination detection methods. However, our findings indicate these attacks are difficult to detect effectively using LLMs alone, highlighting the substantial risks associated with this vulnerability.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    



arXiv:2407.20856
 [pdf, other] 


cs.IR
cs.AI



      
        Learn by Selling: Equipping Large Language Models with Product Knowledge for Context-Driven Recommendations
      
    

Authors:
Sarthak Anand, 
      
      Yutong Jiang, 
      
      Giorgi Kokaia


Abstract:
      
        The rapid evolution of large language models (LLMs) has opened up new possibilities for applications such as context-driven product recommendations. However, the effectiveness of these models in this context is heavily reliant on their comprehensive understanding of the product inventory. This paper presents a novel approach to equipping…
        ▽ More


        The rapid evolution of large language models (LLMs) has opened up new possibilities for applications such as context-driven product recommendations. However, the effectiveness of these models in this context is heavily reliant on their comprehensive understanding of the product inventory. This paper presents a novel approach to equipping LLMs with product knowledge by training them to respond contextually to synthetic search queries that include product IDs. We delve into an extensive analysis of this method, evaluating its effectiveness, outlining its benefits, and highlighting its constraints. The paper also discusses the potential improvements and future directions for this approach, providing a comprehensive understanding of the role of LLMs in product recommendations.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    



arXiv:2407.20840
 [pdf, other] 


cs.NI



      
        Large Language Model (LLM)-enabled Graphs in Dynamic Networking
      
    

Authors:
Geng Sun, 
      
      Yixian Wang, 
      
      Dusit Niyato, 
      
      Jiacheng Wang, 
      
      Xinying Wang, 
      
      H. Vincent Poor, 
      
      Khaled B. Letaief


Abstract:
      
        Recent advances in generative artificial intelligence (AI), and particularly the integration of large language models (LLMs), have had considerable impact on multiple domains. Meanwhile, enhancing dynamic network performance is a crucial element in promoting technological advancement and meeting the growing demands of users in many applications areas involvi…
        ▽ More


        Recent advances in generative artificial intelligence (AI), and particularly the integration of large language models (LLMs), have had considerable impact on multiple domains. Meanwhile, enhancing dynamic network performance is a crucial element in promoting technological advancement and meeting the growing demands of users in many applications areas involving networks. In this article, we explore an integration of LLMs and graphs in dynamic networks, focusing on potential applications and a practical study. Specifically, we first review essential technologies and applications of LLM-enabled graphs, followed by an exploration of their advantages in dynamic networking. Subsequently, we introduce and analyze LLM-enabled graphs and their applications in dynamic networks from the perspective of LLMs as different roles. On this basis, we propose a novel framework of LLM-enabled graphs for networking optimization, and then present a case study on UAV networking, concentrating on optimizing UAV trajectory and communication resource allocation to validate the effectiveness of the proposed framework. Finally, we outline several potential future extensions.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    

Comments:
10 pages, 6 figures, published to IEEE NETWORK




arXiv:2407.20828
 [pdf, other] 


cs.AI
cs.LG



      
        How to Measure the Intelligence of Large Language Models?
      
    

Authors:
Nils Körber, 
      
      Silvan Wehrli, 
      
      Christopher Irrgang


Abstract:
      
        With the release of ChatGPT and other large language models (LLMs) the discussion about the intelligence, possibilities, and risks, of current and future models have seen large attention. This discussion included much debated scenarios about the imminent rise of so-called "super-human" AI, i.e., AI systems that are orders of magnitude smarter than hu…
        ▽ More


        With the release of ChatGPT and other large language models (LLMs) the discussion about the intelligence, possibilities, and risks, of current and future models have seen large attention. This discussion included much debated scenarios about the imminent rise of so-called "super-human" AI, i.e., AI systems that are orders of magnitude smarter than humans. In the spirit of Alan Turing, there is no doubt that current state-of-the-art language models already pass his famous test. Moreover, current models outperform humans in several benchmark tests, so that publicly available LLMs have already become versatile companions that connect everyday life, industry and science. Despite their impressive capabilities, LLMs sometimes fail completely at tasks that are thought to be trivial for humans. In other cases, the trustworthiness of LLMs becomes much more elusive and difficult to evaluate. Taking the example of academia, language models are capable of writing convincing research articles on a given topic with only little input. Yet, the lack of trustworthiness in terms of factual consistency or the existence of persistent hallucinations in AI-generated text bodies has led to a range of restrictions for AI-based content in many scientific journals. In view of these observations, the question arises as to whether the same metrics that apply to human intelligence can also be applied to computational methods and has been discussed extensively. In fact, the choice of metrics has already been shown to dramatically influence assessments on potential intelligence emergence. Here, we argue that the intelligence of LLMs should not only be assessed by task-specific statistical metrics, but separately in terms of qualitative and quantitative measures.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    

Comments:
3 pages, 1 figure




arXiv:2407.20729
 [pdf, other] 


cs.CL



      
        Adapting Safe-for-Work Classifier for Malaysian Language Text: Enhancing Alignment in LLM-Ops Framework
      
    

Authors:
Aisyah Razak, 
      
      Ariff Nazhan, 
      
      Kamarul Adha, 
      
      Wan Adzhar Faiq Adzlan, 
      
      Mas Aisyah Ahmad, 
      
      Ammar Azman


Abstract:
      
        As large language models (LLMs) become increasingly integrated into operational workflows (…
        ▽ More


        As large language models (LLMs) become increasingly integrated into operational workflows (LLM-Ops), there is a pressing need for effective guardrails to ensure safe and aligned interactions, including the ability to detect potentially unsafe or inappropriate content across languages. However, existing safe-for-work classifiers are primarily focused on English text. To address this gap for the Malaysian language, we present a novel safe-for-work text classifier tailored specifically for Malaysian language content. By curating and annotating a first-of-its-kind dataset of Malaysian text spanning multiple content categories, we trained a classification model capable of identifying potentially unsafe material using state-of-the-art natural language processing techniques. This work represents an important step in enabling safer interactions and content filtering to mitigate potential risks and ensure responsible deployment of LLMs. To maximize accessibility and promote further research towards enhancing alignment in LLM-Ops for the Malaysian context, the model is publicly released at https://huggingface.co/malaysia-ai/malaysian-sfw-classifier.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    



arXiv:2407.20712
 [pdf, other] 


cs.HC
cs.AI



      
        Cocobo: Exploring Large Language Models as the Engine for End-User Robot Programming
      
    

Authors:
Yate Ge, 
      
      Yi Dai, 
      
      Run Shan, 
      
      Kechun Li, 
      
      Yuanda Hu, 
      
      Xiaohua Sun


Abstract:
      
        …an expansive user expression space and limited support for debugging and editing, which restrict its application in end-user programming. The emergence of large language models (LLMs) offers promising avenues for the translation and interpretation between human language instructions and the code executed by robots, but their application in end-user programmi…
        ▽ More


        End-user development allows everyday users to tailor service robots or applications to their needs. One user-friendly approach is natural language programming. However, it encounters challenges such as an expansive user expression space and limited support for debugging and editing, which restrict its application in end-user programming. The emergence of large language models (LLMs) offers promising avenues for the translation and interpretation between human language instructions and the code executed by robots, but their application in end-user programming systems requires further study. We introduce Cocobo, a natural language programming system with interactive diagrams powered by LLMs. Cocobo employs LLMs to understand users' authoring intentions, generate and explain robot programs, and facilitate the conversion between executable code and flowchart representations. Our user study shows that Cocobo has a low learning curve, enabling even users with zero coding experience to customize robot programs successfully.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    

Comments:
This is the preprint version of a paper accepted for presentation at the IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC), 2024




arXiv:2407.20668
 [pdf] 


cs.AI



      
        Mimicking the Mavens: Agent-based Opinion Synthesis and Emotion Prediction for Social Media Influencers
      
    

Authors:
Qinglan Wei, 
      
      Ruiqi Xue, 
      
      Yutian Wang, 
      
      Hongjiang Xiao, 
      
      Yuhao Wang, 
      
      Xiaoyan Duan


Abstract:
      
        …and trending topics. We then build a total of 60 anonymous opinion leader agents in six domains and realize the views generation based on an enhanced large language model (LLM) coupled with retrieval-augmented generation (RAG). Subsequently, we synthesize the potential views of opinion leaders and predicted the emotional responses to different events. The ef…
        ▽ More


        Predicting influencers' views and public sentiment on social media is crucial for anticipating societal trends and guiding strategic responses. This study introduces a novel computational framework to predict opinion leaders' perspectives and the emotive reactions of the populace, addressing the inherent challenges posed by the unstructured, context-sensitive, and heterogeneous nature of online communication. Our research introduces an innovative module that starts with the automatic 5W1H (Where, Who, When, What, Why, and How) questions formulation engine, tailored to emerging news stories and trending topics. We then build a total of 60 anonymous opinion leader agents in six domains and realize the views generation based on an enhanced large language model (LLM) coupled with retrieval-augmented generation (RAG). Subsequently, we synthesize the potential views of opinion leaders and predicted the emotional responses to different events. The efficacy of our automated 5W1H module is corroborated by an average GPT-4 score of 8.83/10, indicative of high fidelity. The influencer agents exhibit a consistent performance, achieving an average GPT-4 rating of 6.85/10 across evaluative metrics. Utilizing the 'Russia-Ukraine War' as a case study, our methodology accurately foresees key influencers' perspectives and aligns emotional predictions with real-world sentiment trends in various domains.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    

Comments:
Upon acceptance of the article by IEEE, the preprint article must be replaced with the accepted version, as described in the section 'Accepted article.'




arXiv:2407.20654
 [pdf, other] 


cs.CL
cs.AI



      
        Prompting Encoder Models for Zero-Shot Classification: A Cross-Domain Study in Italian
      
    

Authors:
Serena Auriemma, 
      
      Martina Miliani, 
      
      Mauro Madeddu, 
      
      Alessandro Bondielli, 
      
      Lucia Passaro, 
      
      Alessandro Lenci


Abstract:
      
        …challenge of limited annotated data in specialized fields and low-resource languages is crucial for the effective use of Language Models (LMs). While most Large Language Models (LLMs) are trained on general-purpose English corpora, there is a notable gap in models specifically tailored for Italian, particularly for technical and bureaucratic jargon. This pap…
        ▽ More


        Addressing the challenge of limited annotated data in specialized fields and low-resource languages is crucial for the effective use of Language Models (LMs). While most Large Language Models (LLMs) are trained on general-purpose English corpora, there is a notable gap in models specifically tailored for Italian, particularly for technical and bureaucratic jargon. This paper explores the feasibility of employing smaller, domain-specific encoder LMs alongside prompting techniques to enhance performance in these specialized contexts. Our study concentrates on the Italian bureaucratic and legal language, experimenting with both general-purpose and further pre-trained encoder-only models. We evaluated the models on downstream tasks such as document classification and entity typing and conducted intrinsic evaluations using Pseudo-Log-Likelihood. The results indicate that while further pre-trained models may show diminished robustness in general knowledge, they exhibit superior adaptability for domain-specific tasks, even in a zero-shot setting. Furthermore, the application of calibration techniques and in-domain verbalizers significantly enhances the efficacy of encoder models. These domain-specialized models prove to be particularly advantageous in scenarios where in-domain resources or expertise are scarce. In conclusion, our findings offer new insights into the use of Italian models in specialized contexts, which may have a significant impact on both research and industrial applications in the digital transformation era.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    

Comments:
Submitted to 'Language Resource and Evaluation'


MSC Class:
          68T50; 68T07
        

        
          ACM Class:
          I.2.7
        
      



arXiv:2407.20608
 [pdf, other] 


cs.HC
cs.CL



      
        Questionnaires for Everyone: Streamlining Cross-Cultural Questionnaire Adaptation with GPT-Based Translation Quality Evaluation
      
    

Authors:
Otso Haavisto, 
      
      Robin Welsch


Abstract:
      
        …of the translations created using the tool, evaluation scores between conventionally translated and tool-supported versions were compared. Our results indicate that integrating LLM-generated translation quality evaluations and suggestions for improvement can help users independently attain results similar to those provided by conventional, non-NLP-supported…
        ▽ More


        Adapting questionnaires to new languages is a resource-intensive process often requiring the hiring of multiple independent translators, which limits the ability of researchers to conduct cross-cultural research and effectively creates inequalities in research and society. This work presents a prototype tool that can expedite the questionnaire translation process. The tool incorporates forward-backward translation using DeepL alongside GPT-4-generated translation quality evaluations and improvement suggestions. We conducted two online studies in which participants translated questionnaires from English to either German (Study 1; n=10) or Portuguese (Study 2; n=20) using our prototype. To evaluate the quality of the translations created using the tool, evaluation scores between conventionally translated and tool-supported versions were compared. Our results indicate that integrating LLM-generated translation quality evaluations and suggestions for improvement can help users independently attain results similar to those provided by conventional, non-NLP-supported translation methods. This is the first step towards more equitable questionnaire-based research, powered by AI.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    

Comments:
19 pages, 13 figures




arXiv:2407.20588
 [pdf, ps, other] 


cs.CL



      
        Enhancing Agricultural Machinery Management through Advanced LLM Integration
      
    

Authors:
Emily Johnson, 
      
      Noah Wilson


Abstract:
      
        …Management (CIAMM), has the potential to revolutionize efficiency and sustainability in farming. This paper introduces a novel approach that leverages large language models (LLMs), particularly GPT-4, combined with multi-round prompt engineering to enhance decision-making processes in agricultural machinery management. We systematically developed and refined…
        ▽ More


        The integration of artificial intelligence into agricultural practices, specifically through Consultation on Intelligent Agricultural Machinery Management (CIAMM), has the potential to revolutionize efficiency and sustainability in farming. This paper introduces a novel approach that leverages large language models (LLMs), particularly GPT-4, combined with multi-round prompt engineering to enhance decision-making processes in agricultural machinery management. We systematically developed and refined prompts to guide the LLMs in generating precise and contextually relevant outputs. Our approach was evaluated using a manually curated dataset from various online sources, and performance was assessed with accuracy and GPT-4 Scores. Comparative experiments were conducted using LLama-2-70B, ChatGPT, and GPT-4 models, alongside baseline and state-of-the-art methods such as Chain of Thought (CoT) and Thought of Thought (ThoT). The results demonstrate that our method significantly outperforms these approaches, achieving higher accuracy and relevance in generated responses. This paper highlights the potential of advanced prompt engineering techniques in improving the robustness and applicability of AI in agricultural contexts.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    

Comments:
10 pages




arXiv:2407.20584
 [pdf, other] 


cs.CL
cs.AI



      
        Pruning Large Language Models with Semi-Structural Adaptive Sparse Training
      
    

Authors:
Weiyu Huang, 
      
      Guohao Jian, 
      
      Yuezhou Hu, 
      
      Jun Zhu, 
      
      Jianfei Chen


Abstract:
      
        Transformer-based Large Language Models (LLMs) have demonstrated remarkable success across various challenging tasks. However, the deployment of…
        ▽ More


        Transformer-based Large Language Models (LLMs) have demonstrated remarkable success across various challenging tasks. However, the deployment of LLMs is hindered by their substantial parameter count and memory consumption. Recently, numerous studies have attempted to compress LLMs by pruning them using training-free methods. However, these pruned models often experience significant performance degradation on complex tasks. To address this issue, we propose a novel training pipeline for semi-structured sparse models, named Adaptive Sparse Trainer (AST). By distilling the knowledge stored in its dense counterpart, we prevent the sparse model from overfitting and ensure a stable training process. Moreover, AST allows the model to adaptively select better lottery tickets (e.g., masks) during training. Additionally, we discovered that adding extra well-initialized parameters can further enhance model performance with only a small increase in memory footprint. Our method significantly narrows the performance gap between dense and sparse models while maintaining limited computational cost. Furthermore, when combined with existing quantization methods, AST can compress language models by up to 16x compared to dense FP32 precision models with minimal performance loss. AST outperforms previous state-of-the-art methods by reducing the zero-shot accuracy gap between dense and semi-structured sparse models to 1.12% across multiple zero-shot tasks on Llama2-7B, using less than 0.4% of the pretraining tokens.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    



arXiv:2407.20578
 [pdf, ps, other] 


cs.CL
cs.AI
cs.CY



      
        Comparison of Large Language Models for Generating Contextually Relevant Questions
      
    

Authors:
Ivo Lodovico Molina, 
      
      Valdemar Švábenský, 
      
      Tsubasa Minematsu, 
      
      Li Chen, 
      
      Fumiya Okubo, 
      
      Atsushi Shimada


Abstract:
      
        This study explores the effectiveness of Large Language Models (LLMs) for Automatic Question Generation in educational settings. Three…
        ▽ More


        This study explores the effectiveness of Large Language Models (LLMs) for Automatic Question Generation in educational settings. Three LLMs are compared in their ability to create questions from university slide text without fine-tuning. Questions were obtained in a two-step pipeline: first, answer phrases were extracted from slides using Llama 2-Chat 13B; then, the three models generated questions for each answer. To analyze whether the questions would be suitable in educational applications for students, a survey was conducted with 46 students who evaluated a total of 246 questions across five metrics: clarity, relevance, difficulty, slide relation, and question-answer alignment. Results indicate that GPT-3.5 and Llama 2-Chat 13B outperform Flan T5 XXL by a small margin, particularly in terms of clarity and question-answer alignment. GPT-3.5 especially excels at tailoring questions to match the input answers. The contribution of this research is the analysis of the capacity of LLMs for Automatic Question Generation in education.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    

Comments:
Published in Springer ECTEL 2024 conference proceedings


ACM Class:
          K.3
        
      



arXiv:2407.20570
 [pdf, other] 


cs.HC



      
        Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education
      
    

Authors:
Lin Gao, 
      
      Jing Lu, 
      
      Zekai Shao, 
      
      Ziyue Lin, 
      
      Shengbin Yue, 
      
      Chiokit Ieong, 
      
      Yi Sun, 
      
      Rory James Zauner, 
      
      Zhongyu Wei, 
      
      Siming Chen


Abstract:
      
        Large Language Models (LLMs) have shown great potential in intelligent visualization systems, especially for domain-specific applications. Integrating…
        ▽ More


        Large Language Models (LLMs) have shown great potential in intelligent visualization systems, especially for domain-specific applications. Integrating LLMs into visualization systems presents challenges, and we categorize these challenges into three alignments: domain problems with LLMs, visualization with LLMs, and interaction with LLMs. To achieve these alignments, we propose a framework and outline a workflow to guide the application of fine-tuned LLMs to enhance visual interactions for domain-specific tasks. These alignment challenges are critical in education because of the need for an intelligent visualization system to support beginners' self-regulated learning. Therefore, we apply the framework to education and introduce Tailor-Mind, an interactive visualization system designed to facilitate self-regulated learning for artificial intelligence beginners. Drawing on insights from a preliminary study, we identify self-regulated learning tasks and fine-tuning objectives to guide visualization design and tuning data construction. Our focus on aligning visualization with fine-tuned LLM makes Tailor-Mind more like a personalized tutor. Tailor-Mind also supports interactive recommendations to help beginners better achieve their learning goals. Model performance evaluations and user studies confirm that Tailor-Mind improves the self-regulated learning experience, effectively validating the proposed framework.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    



arXiv:2407.20564
 [pdf, other] 


cs.CL



      
        CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large Language Models over Factual Knowledge
      
    

Authors:
Tianshi Zheng, 
      
      Jiaxin Bai, 
      
      Yicheng Wang, 
      
      Tianqing Fang, 
      
      Yue Guo, 
      
      Yauwai Yim, 
      
      Yangqiu Song


Abstract:
      
        While large language models (LLMs) have demonstrated impressive capabilities across various natural language processing tasks by acquiring rich factual knowledge from their broad training data, their ability to synthesize and logically reason with this knowledge in complex ways remains underexplored. In this work, we present a systematic evaluation of state-…
        ▽ More


        While large language models (LLMs) have demonstrated impressive capabilities across various natural language processing tasks by acquiring rich factual knowledge from their broad training data, their ability to synthesize and logically reason with this knowledge in complex ways remains underexplored. In this work, we present a systematic evaluation of state-of-the-art LLMs' complex logical reasoning abilities through a novel benchmark of automatically generated complex reasoning questions over general domain and biomedical knowledge graphs. Our extensive experiments, employing diverse in-context learning techniques, reveal that LLMs excel at reasoning over general world knowledge but face significant challenges with specialized domain-specific knowledge. We find that prompting with explicit Chain-of-Thought demonstrations can substantially improve LLM performance on complex logical reasoning tasks with diverse logical operations. Interestingly, our controlled evaluations uncover an asymmetry where LLMs display proficiency at set union operations, but struggle considerably with set intersections - a key building block of logical reasoning. To foster further work, we will publicly release our evaluation benchmark and code.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    

Comments:
9 pages




arXiv:2407.20563
 [pdf, other] 


cs.CV
cs.AI



      
        Pyramid Coder: Hierarchical Code Generator for Compositional Visual Question Answering
      
    

Authors:
Ruoyue Shen, 
      
      Nakamasa Inoue, 
      
      Koichi Shinoda


Abstract:
      
        …accurate answers to natural language questions based on visual input. Programmatic VQA (PVQA) models have been gaining attention recently. These use large language models (LLMs) to formulate executable programs that address questions requiring complex visual reasoning. However, there are challenges in enabling…
        ▽ More


        Visual question answering (VQA) is the task of providing accurate answers to natural language questions based on visual input. Programmatic VQA (PVQA) models have been gaining attention recently. These use large language models (LLMs) to formulate executable programs that address questions requiring complex visual reasoning. However, there are challenges in enabling LLMs to comprehend the usage of image processing modules and generate relevant code. To overcome these challenges, this paper introduces PyramidCoder, a novel prompting framework for PVQA models. PyramidCoder consists of three hierarchical levels, each serving a distinct purpose: query rephrasing, code generation, and answer aggregation. Notably, PyramidCoder utilizes a single frozen LLM and pre-defined prompts at each level, eliminating the need for additional training and ensuring flexibility across various LLM architectures. Compared to the state-of-the-art PVQA model, our approach improves accuracy by at least 0.5% on the GQA dataset, 1.4% on the VQAv2 dataset, and 2.9% on the NLVR2 dataset.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    

Comments:
Accepted to the IEEE International Conference on Image Processing (IEEE ICIP) 2024




arXiv:2407.20557
 [pdf] 


cs.LG
cs.AI



      
        CELLM: An Efficient Communication in Large Language Models Training for Federated Learning
      
    

Authors:
Raja Vavekanand, 
      
      Kira Sam


Abstract:
      
        …communicates and aggregates data. However, FL training suffers from statistical heterogeneity as clients may have differing local data distributions. Large language models (LLMs) offer a potential solution to this issue of heterogeneity given that they have consistently been shown to be able to learn on vast amounts of noisy data. While…
        ▽ More


        Federated Learning (FL) is a recent model training paradigm in which client devices collaboratively train a model without ever aggregating their data. Crucially, this scheme offers users potential privacy and security benefits by only ever communicating updates to the model weights to a central server as opposed to traditional machine learning (ML) training which directly communicates and aggregates data. However, FL training suffers from statistical heterogeneity as clients may have differing local data distributions. Large language models (LLMs) offer a potential solution to this issue of heterogeneity given that they have consistently been shown to be able to learn on vast amounts of noisy data. While LLMs are a promising development for resolving the consistent issue of non-I.I.D. Clients in federated settings exacerbate two other bottlenecks in FL: limited local computing and expensive communication. This thesis aims to develop efficient training methods for LLMs in FL. To this end, we employ two critical techniques in enabling efficient training. First, we use low-rank adaptation (LoRA) to reduce the computational load of local model training. Second, we communicate sparse updates throughout training to significantly cut down on communication costs. Taken together, our method reduces communication costs by up to 10x over vanilla LoRA and up to 5x over more complex sparse LoRA baselines while achieving greater utility. We emphasize the importance of carefully applying sparsity and picking effective rank and sparsity configurations for federated LLM training.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    

Comments:
22 pages, 10 figures




arXiv:2407.20534
 [pdf, ps, other] 


q-bio.OT



      
        BERT and LLMs-Based avGFP Brightness Prediction and Mutation Design
      
    

Authors:
X. Guo, 
      
      W. Che


Abstract:
      
        This study aims to utilize Transformer models and large language models (such as GPT and Claude) to predict the brightness of Aequorea victoria green fluorescent protein (avGFP) and design mutants with higher brightness. Considering the time and cost associated with traditional experimental screening methods, this study employs machine learning techniques to enhance research efficiency. We first r…
        ▽ More


        This study aims to utilize Transformer models and large language models (such as GPT and Claude) to predict the brightness of Aequorea victoria green fluorescent protein (avGFP) and design mutants with higher brightness. Considering the time and cost associated with traditional experimental screening methods, this study employs machine learning techniques to enhance research efficiency. We first read and preprocess a proprietary dataset containing approximately 140,000 protein sequences, including about 30,000 avGFP sequences. Subsequently, we constructed and trained a Transformer-based prediction model to screen and design new avGFP mutants that are expected to exhibit higher brightness.
  Our methodology consists of two primary stages: first, the construction of a scoring model using BERT, and second, the screening and generation of mutants using mutation site statistics and large language models. Through the analysis of predictive results, we designed and screened 10 new high-brightness avGFP sequences. This study not only demonstrates the potential of deep learning in protein design but also provides new perspectives and methodologies for future research by integrating prior knowledge from large language models.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    



arXiv:2407.20529
 [pdf, other] 


cs.LG
cs.CR



      
        Can LLMs be Fooled? Investigating Vulnerabilities in LLMs


Authors:
Sara Abdali, 
      
      Jia He, 
      
      CJ Barberan, 
      
      Richard Anarfi


Abstract:
      
        The advent of Large Language Models (LLMs) has garnered significant popularity and wielded immense power across various domains within Natural Language Processing (NLP). While their capabilities are undeniably impressive, it is crucial to identify and scrutinize their vulnerabilities especially when those vulnerabilities can have costly consequences. One suc…
        ▽ More


        The advent of Large Language Models (LLMs) has garnered significant popularity and wielded immense power across various domains within Natural Language Processing (NLP). While their capabilities are undeniably impressive, it is crucial to identify and scrutinize their vulnerabilities especially when those vulnerabilities can have costly consequences. One such LLM, trained to provide a concise summarization from medical documents could unequivocally leak personal patient data when prompted surreptitiously. This is just one of many unfortunate examples that have been unveiled and further research is necessary to comprehend the underlying reasons behind such vulnerabilities. In this study, we delve into multiple sections of vulnerabilities which are model-based, training-time, inference-time vulnerabilities, and discuss mitigation strategies including "Model Editing" which aims at modifying LLMs behavior, and "Chroma Teaming" which incorporates synergy of multiple teaming strategies to enhance LLMs' resilience. This paper will synthesize the findings from each vulnerability section and propose new directions of research and development. By understanding the focal points of current vulnerabilities, we can better anticipate and mitigate future risks, paving the road for more robust and secure LLMs.
        △ Less


Submitted 30 July, 2024; 
      originally announced July 2024.
      
    

Comments:
14 pages, 1 figure. arXiv admin note: text overlap with arXiv:2403.12503




arXiv:2407.20503
 [pdf, other] 


cs.LG
cs.AI



      
        A federated large language model for long-term time series forecasting
      
    

Authors:
Raed Abdel-Sater, 
      
      A. Ben Hamza


Abstract:
      
        …poses unique challenges regarding data privacy, communication overhead, and scalability. To address these challenges, we propose FedTime, a federated large language model (LLM) tailored for long-range time series prediction. Specifically, we introduce a federated pre-trained LLM with fine-tuning and alignment strategie…
        ▽ More


        Long-term time series forecasting in centralized environments poses unique challenges regarding data privacy, communication overhead, and scalability. To address these challenges, we propose FedTime, a federated large language model (LLM) tailored for long-range time series prediction. Specifically, we introduce a federated pre-trained LLM with fine-tuning and alignment strategies. Prior to the learning process, we employ K-means clustering to partition edge devices or clients into distinct clusters, thereby facilitating more focused model training. We also incorporate channel independence and patching to better preserve local semantic information, ensuring that important contextual details are retained while minimizing the risk of information loss. We demonstrate the effectiveness of our FedTime model through extensive experiments on various real-world forecasting benchmarks, showcasing substantial improvements over recent approaches. In addition, we demonstrate the efficiency of FedTime in streamlining resource usage, resulting in reduced communication overhead.
        △ Less


Submitted 29 July, 2024; 
      originally announced July 2024.
      
    



arXiv:2407.20485
 [pdf, other] 


cs.CL
cs.LG



      
        A2SF: Accumulative Attention Scoring with Forgetting Factor for Token Pruning in Transformer Decoder
      
    

Authors:
Hyun Rae Jo, 
      
      Dong Kun Shin


Abstract:
      
        Recently, large language models (LLM) based on transformers are facing memory bottleneck issues due to KV cache, especially in long sequence handling. Previous researches proposed KV cache compression techniques that identify insignificant tokens based on Accumulative Attention Scores and removes their items from KV cache, noting that only few tokens play an…
        ▽ More


        Recently, large language models (LLM) based on transformers are facing memory bottleneck issues due to KV cache, especially in long sequence handling. Previous researches proposed KV cache compression techniques that identify insignificant tokens based on Accumulative Attention Scores and removes their items from KV cache, noting that only few tokens play an important role in attention operations. However, we have observed that the existing Accumulative Attention Score is not suitable for the transformer decoder structure. In the decoder model, the number of times the Attention Score accumulates varies depending on the order of token appearance due to the effect of masking, causing an uneven comparison between tokens. To solve this, we propose Accumulative Attention Score with Forgetting Factor (A2SF) technique, which introduces a Forgetting Factor in the Attention Score accumulation process. A2SF applies a penalty to the past Attention Score generated from old tokens by repeatedly multiplying the Forgetting Factor to the Attention Score over time. Therefore, older tokens receive a larger penalty, providing fairness among different ages of tokens. Through the fair comparison among tokens, we can more effectively select important tokens. We have verified the accuracy improvement through A2SF in the OPT and LLaMA models and A2SF improves the accuracy of LLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.
        △ Less


Submitted 29 July, 2024; 
      originally announced July 2024.
      
    

Comments:
11 pages(9 pages + reference 2 pages), 6 figures




arXiv:2407.20454
 [pdf, other] 


cs.LG
cs.CL



      
        CoMMIT: Coordinated Instruction Tuning for Multimodal Large Language Models
      
    

Authors:
Junda Wu, 
      
      Xintong Li, 
      
      Tong Yu, 
      
      Yu Wang, 
      
      Xiang Chen, 
      
      Jiuxiang Gu, 
      
      Lina Yao, 
      
      Jingbo Shang, 
      
      Julian McAuley


Abstract:
      
        Instruction tuning in multimodal large language models (MLLMs) aims to smoothly integrate a backbone LLM with a pre-trained feature encoder for downstream tasks. The major challenge is how to efficiently find the synergy through cooperative learning where…
        ▽ More


        Instruction tuning in multimodal large language models (MLLMs) aims to smoothly integrate a backbone LLM with a pre-trained feature encoder for downstream tasks. The major challenge is how to efficiently find the synergy through cooperative learning where LLMs adapt their reasoning abilities in downstream tasks while feature encoders adjust their encoding to provide more relevant modal information. In this paper, we analyze the MLLM instruction tuning from both theoretical and empirical perspectives, where we find unbalanced learning between the two components, i.e., the feature encoder and the LLM, can cause diminishing learning gradients that slow the model convergence and often lead to sub-optimal results due to insufficient learning. Inspired by our findings, we propose a measurement to quantitatively evaluate the learning balance, based on which we further design a dynamic learning scheduler that better coordinates the learning. In addition, we introduce an auxiliary loss regularization method to promote updating of the generation distribution of MLLMs considering the learning state of each model component, which potentially prevents each component from gradient diminishing and enables a more accurate estimation of the learning balance coefficient. We conduct experiments with multiple LLM backbones and feature encoders, where our techniques are model-agnostic and can be generically integrated with various MLLM backbones. Experiment results on multiple downstream tasks and modalities in vision and audio, demonstrate the proposed method's better efficiency and effectiveness in MLLM instruction tuning.
        △ Less


Submitted 29 July, 2024; 
      originally announced July 2024.
      
    

Comments:
9 pages




arXiv:2407.20445
 [pdf, other] 


cs.SD
cs.AI
cs.LG
eess.AS



      
        Futga: Towards Fine-grained Music Understanding through Temporally-enhanced Generative Augmentation
      
    

Authors:
Junda Wu, 
      
      Zachary Novack, 
      
      Amit Namburi, 
      
      Jiaheng Dai, 
      
      Hao-Wen Dong, 
      
      Zhouhang Xie, 
      
      Carol Chen, 
      
      Julian McAuley


Abstract:
      
        …music understanding capabilities through learning from generative augmentation with temporal compositions. We leverage existing music caption datasets and large language models (LLMs) to synthesize fine-grained music captions with structural descriptions and time boundaries for full-length songs. Augmented by the proposed synthetic dataset, FUTGA is enabled…
        ▽ More


        Existing music captioning methods are limited to generating concise global descriptions of short music clips, which fail to capture fine-grained musical characteristics and time-aware musical changes. To address these limitations, we propose FUTGA, a model equipped with fined-grained music understanding capabilities through learning from generative augmentation with temporal compositions. We leverage existing music caption datasets and large language models (LLMs) to synthesize fine-grained music captions with structural descriptions and time boundaries for full-length songs. Augmented by the proposed synthetic dataset, FUTGA is enabled to identify the music's temporal changes at key transition points and their musical functions, as well as generate detailed descriptions for each music segment. We further introduce a full-length music caption dataset generated by FUTGA, as the augmentation of the MusicCaps and the Song Describer datasets. We evaluate the automatically generated captions on several downstream tasks, including music generation and retrieval. The experiments demonstrate the quality of the generated captions and the better performance in various downstream tasks achieved by the proposed music captioning approach. Our code and datasets can be found in \href{https://huggingface.co/JoshuaW1997/FUTGA}{\textcolor{blue}{https://huggingface.co/JoshuaW1997/FUTGA}}.
        △ Less


Submitted 29 July, 2024; 
      originally announced July 2024.
      
    

Comments:
6 pages




arXiv:2407.20413
 [pdf, other] 


cs.CL



      
        Through the Looking Glass, and what Horn Clause Programs Found There
      
    

Authors:
Paul Tarau


Abstract:
      
        …with no performance penalty and we design the embedded SymLP language to support combined Horn clause and Dual Horn clause programs.
  As a (motivating) application, we cast LLM reasoning chains into propositional Horn and Dual Horn clauses that work together to constructively prove and disprove goals and enhance Generative AI with explainability of reasonin…
        ▽ More


        Dual Horn clauses mirror key properties of Horn clauses. This paper explores the ``other side of the looking glass'' to reveal some expected and unexpected symmetries and their practical uses.
  We revisit Dual Horn clauses as enablers of a form of constructive negation that supports goal-driven forward reasoning and is valid both intuitionistically and classically. In particular, we explore the ability to falsify a counterfactual hypothesis in the context of a background theory expressed as a Dual Horn clause program.
  With Dual Horn clause programs, by contrast to negation as failure, the variable bindings in their computed answers provide explanations for the reasons why a statement is successfully falsified. Moreover, in the propositional case, by contrast to negation as failure as implemented with stable models semantics in ASP systems, and similarly to Horn clause programs, Dual Horn clause programs have polynomial complexity.
  After specifying their execution model with a metainterpreter, we devise a compilation scheme from Dual Horn clause programs to Horn clause programs, ensuring their execution with no performance penalty and we design the embedded SymLP language to support combined Horn clause and Dual Horn clause programs.
  As a (motivating) application, we cast LLM reasoning chains into propositional Horn and Dual Horn clauses that work together to constructively prove and disprove goals and enhance Generative AI with explainability of reasoning chains.
        △ Less


Submitted 29 July, 2024; 
      originally announced July 2024.
      
    



arXiv:2407.20382
 [pdf, other] 


cs.CL



      
        What if Red Can Talk? Dynamic Dialogue Generation Using Large Language Models
      
    

Authors:
Navapat Nananukul, 
      
      Wichayaporn Wongkamjan


Abstract:
      
        …main story and character personalities, player immersion can be significantly enhanced through casual interactions between characters. With the advent of large language models (LLMs), we introduce a dialogue filler framework that utilizes LLMs enhanced by knowledge graphs to generate dynamic and contextually appropriat…
        ▽ More


        Role-playing games (RPGs) provide players with a rich, interactive world to explore. Dialogue serves as the primary means of communication between developers and players, manifesting in various forms such as guides, NPC interactions, and storytelling. While most games rely on written scripts to define the main story and character personalities, player immersion can be significantly enhanced through casual interactions between characters. With the advent of large language models (LLMs), we introduce a dialogue filler framework that utilizes LLMs enhanced by knowledge graphs to generate dynamic and contextually appropriate character interactions. We test this framework within the environments of Final Fantasy VII Remake and Pokemon, providing qualitative and quantitative evidence that demonstrates GPT-4's capability to act with defined personalities and generate dialogue. However, some flaws remain, such as GPT-4 being overly positive or more subtle personalities, such as maturity, tend to be of lower quality compared to more overt traits like timidity. This study aims to assist developers in crafting more nuanced filler dialogues, thereby enriching player immersion and enhancing the overall RPG experience.
        △ Less


Submitted 29 July, 2024; 
      originally announced July 2024.
      
    

Comments:
ACL Wordplay 2024




arXiv:2407.20371
 [pdf, other] 


cs.CY
cs.AI
cs.CL
cs.LG



      
        Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval
      
    

Authors:
Kyra Wilson, 
      
      Aylin Caliskan


Abstract:
      
        Artificial intelligence (AI) hiring tools have revolutionized resume screening, and large language models (LLMs) have the potential to do the same. However, given the biases which are embedded within…
        ▽ More


        Artificial intelligence (AI) hiring tools have revolutionized resume screening, and large language models (LLMs) have the potential to do the same. However, given the biases which are embedded within LLMs, it is unclear whether they can be used in this scenario without disadvantaging groups based on their protected attributes. In this work, we investigate the possibilities of using LLMs in a resume screening setting via a document retrieval framework that simulates job candidate selection. Using that framework, we then perform a resume audit study to determine whether a selection of Massive Text Embedding (MTE) models are biased in resume screening scenarios. We simulate this for nine occupations, using a collection of over 500 publicly available resumes and 500 job descriptions. We find that the MTEs are biased, significantly favoring White-associated names in 85.1\% of cases and female-associated names in only 11.1\% of cases, with a minority of cases showing no statistically significant differences. Further analyses show that Black males are disadvantaged in up to 100\% of cases, replicating real-world patterns of bias in employment settings, and validate three hypotheses of intersectionality. We also find an impact of document length as well as the corpus frequency of names in the selection of resumes. These findings have implications for widely used AI tools that are automating employment, fairness, and tech policy.
        △ Less


Submitted 29 July, 2024; 
      originally announced July 2024.
      
    

Comments:
To be published in Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society; code available at https://github.com/kyrawilson/Resume-Screening-Bias


ACM Class:
          K.4.2
        
      



arXiv:2407.20361
 [pdf, other] 


cs.CR



      
        From ML to LLM: Evaluating the Robustness of Phishing Webpage Detection Models against Adversarial Attacks
      
    

Authors:
Aditya Kulkarni, 
      
      Vivek Balachandran, 
      
      Dinil Mon Divakaran, 
      
      Tamal Das


Abstract:
      
        Phishing attacks attempt to deceive users into stealing sensitive information, posing a significant cybersecurity threat. Advances in machine learning (ML) and deep learning (DL) have led to the development of numerous phishing webpage detection solutions, but these models remain vulnerable to adversarial attacks. Evaluating their robustness against adversarial phishing webpages is essential. Exis…
        ▽ More


        Phishing attacks attempt to deceive users into stealing sensitive information, posing a significant cybersecurity threat. Advances in machine learning (ML) and deep learning (DL) have led to the development of numerous phishing webpage detection solutions, but these models remain vulnerable to adversarial attacks. Evaluating their robustness against adversarial phishing webpages is essential. Existing tools contain datasets of pre-designed phishing webpages for a limited number of brands, and lack diversity in phishing features.
  To address these challenges, we develop PhishOracle, a tool that generates adversarial phishing webpages by embedding diverse phishing features into legitimate webpages. We evaluate the robustness of two existing models, Stack model and Phishpedia, in classifying PhishOracle-generated adversarial phishing webpages. Additionally, we study a commercial large language model, Gemini Pro Vision, in the context of adversarial attacks. We conduct a user study to determine whether PhishOracle-generated adversarial phishing webpages deceive users. Our findings reveal that many PhishOracle-generated phishing webpages evade current phishing webpage detection models and deceive users, but Gemini Pro Vision is robust to the attack. We also develop the PhishOracle web app, allowing users to input a legitimate URL, select relevant phishing features and generate a corresponding phishing webpage. All resources are publicly available on GitHub.
        △ Less


Submitted 29 July, 2024; 
      originally announced July 2024.
      
    



arXiv:2407.20360
 [pdf, other] 


cs.AI



      
        Evaluating Large Language Models for automatic analysis of teacher simulations
      
    

Authors:
David de-Fitero-Dominguez, 
      
      Mariano Albaladejo-González, 
      
      Antonio Garcia-Cabot, 
      
      Eva Garcia-Lopez, 
      
      Antonio Moreno-Cediel, 
      
      Erin Barno, 
      
      Justin Reich


Abstract:
      
        …questions, allowing teacher candidates to express their thoughts but complicating an automatic response analysis. To address this issue, we have evaluated Large Language Models (LLMs) to identify characteristics (user behaviors) in the responses of DS for teacher education. We evaluated the performance of DeBERTaV3 and Llama 3, combined with zero-shot, few-s…
        ▽ More


        Digital Simulations (DS) provide safe environments where users interact with an agent through conversational prompts, providing engaging learning experiences that can be used to train teacher candidates in realistic classroom scenarios. These simulations usually include open-ended questions, allowing teacher candidates to express their thoughts but complicating an automatic response analysis. To address this issue, we have evaluated Large Language Models (LLMs) to identify characteristics (user behaviors) in the responses of DS for teacher education. We evaluated the performance of DeBERTaV3 and Llama 3, combined with zero-shot, few-shot, and fine-tuning. Our experiments discovered a significant variation in the LLMs' performance depending on the characteristic to identify. Additionally, we noted that DeBERTaV3 significantly reduced its performance when it had to identify new characteristics. In contrast, Llama 3 performed better than DeBERTaV3 in detecting new characteristics and showing more stable performance. Therefore, in DS where teacher educators need to introduce new characteristics because they change depending on the simulation or the educational objectives, it is more recommended to use Llama 3. These results can guide other researchers in introducing LLMs to provide the highly demanded automatic evaluations in DS.
        △ Less


Submitted 29 July, 2024; 
      originally announced July 2024.
      
    



arXiv:2407.20311
 [pdf, other] 


cs.AI
cs.CL
cs.LG



      
        Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process
      
    

Authors:
Tian Ye, 
      
      Zicheng Xu, 
      
      Yuanzhi Li, 
      
      Zeyuan Allen-Zhu


Abstract:
      
        …questions?
  Our study uncovers many hidden mechanisms by which language models solve mathematical questions, providing insights that extend beyond current understandings of LLMs.
        ▽ More


        Recent advances in language models have demonstrated their capability to solve mathematical reasoning problems, achieving near-perfect accuracy on grade-school level math benchmarks like GSM8K. In this paper, we formally study how language models solve these problems. We design a series of controlled experiments to address several fundamental questions: (1) Can language models truly develop reasoning skills, or do they simply memorize templates? (2) What is the model's hidden (mental) reasoning process? (3) Do models solve math questions using skills similar to or different from humans? (4) Do models trained on GSM8K-like datasets develop reasoning skills beyond those necessary for solving GSM8K problems? (5) What mental process causes models to make reasoning mistakes? (6) How large or deep must a model be to effectively solve GSM8K-level math questions?
  Our study uncovers many hidden mechanisms by which language models solve mathematical questions, providing insights that extend beyond current understandings of LLMs.
        △ Less


Submitted 29 July, 2024; 
      originally announced July 2024.
      
    

Comments:
video appeared in ICML 2024 tutorial




arXiv:2407.20272
 [pdf, other] 


cs.CL
cs.AI
cs.LG



      
        An Efficient Inference Framework for Early-exit Large Language Models
      
    

Authors:
Ruijie Miao, 
      
      Yihan Yan, 
      
      Xinshuo Yao, 
      
      Tong Yang


Abstract:
      
        Building efficient inference framework has gained increasing interests for research community. Early-exit models, a variant of LLMs, improves the inference efficiency of…
        ▽ More


        Building efficient inference framework has gained increasing interests for research community. Early-exit models, a variant of LLMs, improves the inference efficiency of LLMs by skipping rest layers and directly generate output tokens when they are confident enough. However, there is no work of LLM inference framework that takes early-exit models into consideration. This is non-trivial as prior art on LLM inference cannot be directly applied to early-exit models. In this work, we solves two key challenges in building efficient inference framework for early-exit models: (1) batch inference at iteration-level granularity; and (2) KV cache management. For the former, we propose to process the batch until all sequences surpass the early-exit confidence threshold. For the latter, we propose to fill the KV cache of rest layers before the iteration terminates. Our evaluation shows that, compared with the original vLLM operating at full layers, our solution achieves up to 1.25x speed up.
        △ Less


Submitted 25 July, 2024; 
      originally announced July 2024.
      
    



arXiv:2407.20256
 [pdf] 


cs.DB
cs.AI
cs.LG



      
        Making LLMs Work for Enterprise Data Tasks
      
    

Authors:
Çağatay Demiralp, 
      
      Fabian Wenz, 
      
      Peter Baile Chen, 
      
      Moe Kayali, 
      
      Nesime Tatbul, 
      
      Michael Stonebraker


Abstract:
      
        Large language models (LLMs) know little about enterprise database tables in the private data ecosystem, which substantially differ from web text in structure and content. As…
        ▽ More


        Large language models (LLMs) know little about enterprise database tables in the private data ecosystem, which substantially differ from web text in structure and content. As LLMs' performance is tied to their training data, a crucial question is how useful they can be in improving enterprise database management and analysis tasks. To address this, we contribute experimental results on LLMs' performance for text-to-SQL and semantic column-type detection tasks on enterprise datasets. The performance of LLMs on enterprise data is significantly lower than on benchmark datasets commonly used. Informed by our findings and feedback from industry practitioners, we identify three fundamental challenges -- latency, cost, and quality -- and propose potential solutions to use LLMs in enterprise data workflows effectively.
        △ Less


Submitted 22 July, 2024; 
      originally announced July 2024.
      
    

Comments:
Poster at North East Database Day 2024




arXiv:2407.20244
 [pdf, other] 


cs.CL
cs.AI



      
        Steamroller Problems: An Evaluation of LLM Reasoning Capability with Automated Theorem Prover Strategies
      
    

Authors:
Lachlan McGinness, 
      
      Peter Baumgartner


Abstract:
      
        This study presents the first examination of the ability of Large Language Models (LLMs) to follow reasoning strategies that are used to guide Automated Theorem Provers (ATPs). We evaluate the performance of GPT4, GPT3.5 Turbo and Google's recent Gemini model on problems from a steamroller domain. In addition to determining accuracy we make use of the Na…
        ▽ More


        This study presents the first examination of the ability of Large Language Models (LLMs) to follow reasoning strategies that are used to guide Automated Theorem Provers (ATPs). We evaluate the performance of GPT4, GPT3.5 Turbo and Google's recent Gemini model on problems from a steamroller domain. In addition to determining accuracy we make use of the Natural Language Processing library spaCy to explore new methods of investigating LLM's reasoning capabilities. This led to one alarming result, the low correlation between correct reasoning and correct answers for any of the tested models. We found that the models' performance when using the ATP reasoning strategies was comparable to one-shot chain of thought and observe that attention to uncertainty in the accuracy results is critical when drawing conclusions about model performance. Consistent with previous speculation we confirm that LLMs have a preference for, and are best able to follow, bottom up reasoning processes. However, the reasoning strategies can still be beneficial for deriving small and relevant sets of formulas for external processing by a trusted inference engine.
        △ Less


Submitted 17 July, 2024; 
      originally announced July 2024.
      
    



arXiv:2407.20243
 [pdf, other] 


cs.CL
cs.LG



      
        Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions
      
    

Authors:
Jinsung Yoon, 
      
      Raj Sinha, 
      
      Sercan O Arik, 
      
      Tomas Pfister


Abstract:
      
        Embeddings from Large Language Models (LLMs) have emerged as critical components in various applications, particularly for information retrieval. While high-dimensional embeddings generally demonstrate superior performance as they contain more salient information, their practical application is frequently hindered by elevated computational latency and the as…
        ▽ More


        Embeddings from Large Language Models (LLMs) have emerged as critical components in various applications, particularly for information retrieval. While high-dimensional embeddings generally demonstrate superior performance as they contain more salient information, their practical application is frequently hindered by elevated computational latency and the associated higher cost. To address these challenges, we propose Matryoshka-Adaptor, a novel tuning framework designed for the customization of LLM embeddings. Matryoshka-Adaptor facilitates substantial dimensionality reduction while maintaining comparable performance levels, thereby achieving a significant enhancement in computational efficiency and cost-effectiveness. Our framework directly modifies the embeddings from pre-trained LLMs which is designed to be seamlessly integrated with any LLM architecture, encompassing those accessible exclusively through black-box APIs. Also, it exhibits efficacy in both unsupervised and supervised learning settings. A rigorous evaluation conducted across a diverse corpus of English, multilingual, and multimodal datasets consistently reveals substantial gains with Matryoshka-Adaptor. Notably, with Google and OpenAI Embedding APIs, Matryoshka-Adaptor achieves a reduction in dimensionality ranging from two- to twelve-fold without compromising performance across multiple BEIR datasets.
        △ Less


Submitted 17 July, 2024; 
      originally announced July 2024.
      
    



arXiv:2407.20242
 [pdf, other] 


cs.CY
cs.AI
cs.RO



      
        BadRobot: Jailbreaking LLM-based Embodied AI in the Physical World
      
    

Authors:
Hangtao Zhang, 
      
      Chenyu Zhu, 
      
      Xianlong Wang, 
      
      Ziqi Zhou, 
      
      Shengshan Hu, 
      
      Leo Yu Zhang


Abstract:
      
        …and actuators, seamlessly integrating perception and action. This design enables AI to learn from and operate within complex, real-world environments. Large Language Models (LLMs) deeply explore language instructions, playing a crucial role in devising plans for complex tasks. Consequently, they have progressively shown immense potential in empowering embodi…
        ▽ More


        Embodied artificial intelligence (AI) represents an artificial intelligence system that interacts with the physical world through sensors and actuators, seamlessly integrating perception and action. This design enables AI to learn from and operate within complex, real-world environments. Large Language Models (LLMs) deeply explore language instructions, playing a crucial role in devising plans for complex tasks. Consequently, they have progressively shown immense potential in empowering embodied AI, with LLM-based embodied AI emerging as a focal point of research within the community. It is foreseeable that, over the next decade, LLM-based embodied AI robots are expected to proliferate widely, becoming commonplace in homes and industries. However, a critical safety issue that has long been hiding in plain sight is: could LLM-based embodied AI perpetrate harmful behaviors? Our research investigates for the first time how to induce threatening actions in embodied AI, confirming the severe risks posed by these soon-to-be-marketed robots, which starkly contravene Asimov's Three Laws of Robotics and threaten human safety. Specifically, we formulate the concept of embodied AI jailbreaking and expose three critical security vulnerabilities: first, jailbreaking robotics through compromised LLM; second, safety misalignment between action and language spaces; and third, deceptive prompts leading to unaware hazardous behaviors. We also analyze potential mitigation measures and advocate for community awareness regarding the safety of embodied AI applications in the physical world.
        △ Less


Submitted 16 July, 2024; 
      originally announced July 2024.
      
    

Comments:
Preliminary version (15 pages, 4 figures). Work in progress, revisions ongoing. Appreciate understanding and welcome any feedback




arXiv:2407.20240
 [pdf] 


cs.CY
cs.AI



      
        Social and Ethical Risks Posed by General-Purpose LLMs for Settling Newcomers in Canada
      
    

Authors:
Isar Nejadgholi, 
      
      Maryam Molamohammadi


Abstract:
      
        …to first, warn against the unguarded use of generative AI, and second, to incentivize further research and development in creating AI literacy programs as well as customized LLMs that are aligned with the preferences of the impacted communities. Crucially, such technologies should be designed to integrate seamlessly into the existing workflow of the settleme…
        ▽ More


        The non-profit settlement sector in Canada supports newcomers in achieving successful integration. This sector faces increasing operational pressures amidst rising immigration targets, which highlights a need for enhanced efficiency and innovation, potentially through reliable AI solutions. The ad-hoc use of general-purpose generative AI, such as ChatGPT, might become a common practice among newcomers and service providers to address this need. However, these tools are not tailored for the settlement domain and can have detrimental implications for immigrants and refugees. We explore the risks that these tools might pose on newcomers to first, warn against the unguarded use of generative AI, and second, to incentivize further research and development in creating AI literacy programs as well as customized LLMs that are aligned with the preferences of the impacted communities. Crucially, such technologies should be designed to integrate seamlessly into the existing workflow of the settlement sector, ensuring human oversight, trustworthiness, and accountability.
        △ Less


Submitted 15 July, 2024; 
      originally announced July 2024.
      
    

Comments:
26 pages, 8 figures


MSC Class:
          I.2.1; I.2.7
        

        
      



arXiv:2407.20232
 [pdf, other] 


cs.CV
cs.AI
cs.LG



      
        Specify and Edit: Overcoming Ambiguity in Text-Based Image Editing
      
    

Authors:
Ekaterina Iakovleva, 
      
      Fabio Pizzati, 
      
      Philip Torr, 
      
      Stéphane Lathuilière


Abstract:
      
        …(SANE), a zero-shot inference pipeline for diffusion-based editing systems. We use a large language model (LLM) to decompose the input instruction into specific instructions, i.e. well-defined interventions to apply to the input image to satisfy the user's request. We benefit from the LLM-derived instructions alon…
        ▽ More


        Text-based editing diffusion models exhibit limited performance when the user's input instruction is ambiguous. To solve this problem, we propose Specify ANd Edit (SANE), a zero-shot inference pipeline for diffusion-based editing systems. We use a large language model (LLM) to decompose the input instruction into specific instructions, i.e. well-defined interventions to apply to the input image to satisfy the user's request. We benefit from the LLM-derived instructions along the original one, thanks to a novel denoising guidance strategy specifically designed for the task. Our experiments with three baselines and on two datasets demonstrate the benefits of SANE in all setups. Moreover, our pipeline improves the interpretability of editing models, and boosts the output diversity. We also demonstrate that our approach can be applied to any edit, whether ambiguous or not. Our code is public at https://github.com/fabvio/SANE.
        △ Less


Submitted 29 July, 2024; 
      originally announced July 2024.
      
    



arXiv:2407.20224
 [pdf, other] 


cs.CL



      
        Can Editing LLMs Inject Harm?
      
    

Authors:
Canyu Chen, 
      
      Baixiang Huang, 
      
      Zekun Li, 
      
      Zhaorun Chen, 
      
      Shiyang Lai, 
      
      Xiongxiao Xu, 
      
      Jia-Chen Gu, 
      
      Jindong Gu, 
      
      Huaxiu Yao, 
      
      Chaowei Xiao, 
      
      Xifeng Yan, 
      
      William Yang Wang, 
      
      Philip Torr, 
      
      Dawn Song, 
      
      Kai Shu


Abstract:
      
        Knowledge editing techniques have been increasingly adopted to efficiently correct the false or outdated knowledge in Large Language Models (LLMs), due to the high cost of retraining from scratch. Meanwhile, one critical but under-explored question is: can knowledge editing be used to inject harm into…
        ▽ More


        Knowledge editing techniques have been increasingly adopted to efficiently correct the false or outdated knowledge in Large Language Models (LLMs), due to the high cost of retraining from scratch. Meanwhile, one critical but under-explored question is: can knowledge editing be used to inject harm into LLMs? In this paper, we propose to reformulate knowledge editing as a new type of safety threat for LLMs, namely Editing Attack, and conduct a systematic investigation with a newly constructed dataset EditAttack. Specifically, we focus on two typical safety risks of Editing Attack including Misinformation Injection and Bias Injection. For the risk of misinformation injection, we first categorize it into commonsense misinformation injection and long-tail misinformation injection. Then, we find that editing attacks can inject both types of misinformation into LLMs, and the effectiveness is particularly high for commonsense misinformation injection. For the risk of bias injection, we discover that not only can biased sentences be injected into LLMs with high effectiveness, but also one single biased sentence injection can cause a high bias increase in general outputs of LLMs, which are even highly irrelevant to the injected sentence, indicating a catastrophic impact on the overall fairness of LLMs. Then, we further illustrate the high stealthiness of editing attacks, measured by their impact on the general knowledge and reasoning capacities of LLMs, and show the hardness of defending editing attacks with empirical evidence. Our discoveries demonstrate the emerging misuse risks of knowledge editing techniques on compromising the safety alignment of LLMs.
        △ Less


Submitted 29 July, 2024; 
      originally announced July 2024.
      
    

Comments:
The first two authors contributed equally. 9 pages for main paper, 36 pages including appendix. The code, results, dataset for this paper and more resources are on the project website: https://llm-editing.github.io




arXiv:2407.20207
 [pdf, other] 


cs.CL
cs.AI
cs.IR



      
        QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval
      
    

Authors:
Hongming Tan, 
      
      Shaoxiong Zhan, 
      
      Hai Lin, 
      
      Hai-Tao Zheng, 
      
      Wai Kin, 
      
       Chan


Abstract:
      
        …texts to effectively address the aforementioned issues without modifying embedding or retrieval methodologies. Two text representations are generated via large language models (LLMs) zero-shot prompting: question-answer pairs and element-driven events. We term this approach QAEA-DR: unifying question-answer generation and event extraction in a text augmentat…
        ▽ More


        In dense retrieval, embedding long texts into dense vectors can result in information loss, leading to inaccurate query-text matching. Additionally, low-quality texts with excessive noise or sparse key information are unlikely to align well with relevant queries. Recent studies mainly focus on improving the sentence embedding model or retrieval process. In this work, we introduce a novel text augmentation framework for dense retrieval. This framework transforms raw documents into information-dense text formats, which supplement the original texts to effectively address the aforementioned issues without modifying embedding or retrieval methodologies. Two text representations are generated via large language models (LLMs) zero-shot prompting: question-answer pairs and element-driven events. We term this approach QAEA-DR: unifying question-answer generation and event extraction in a text augmentation framework for dense retrieval. To further enhance the quality of generated texts, a scoring-based evaluation and regeneration mechanism is introduced in LLM prompting. Our QAEA-DR model has a positive impact on dense retrieval, supported by both theoretical analysis and empirical experiments.
        △ Less


Submitted 29 July, 2024; 
      originally announced July 2024.
      
    



arXiv:2407.20183
 [pdf, other] 


cs.CL
cs.AI



      
        MindSearch: Mimicking Human Minds Elicits Deep AI Searcher
      
    

Authors:
Zehui Chen, 
      
      Kuikun Liu, 
      
      Qiuchen Wang, 
      
      Jiangning Liu, 
      
      Wenwei Zhang, 
      
      Kai Chen, 
      
      Feng Zhao


Abstract:
      
        …complex cognitive task that consumes enormous time and effort. Inspired by the remarkable progress of Large Language Models, recent works attempt to solve this task by combining LLMs and search engines. However, these methods still obtain unsatisfying performance due to three challenges: (1) complex requests often cannot be accurately and completely retrieve…
        ▽ More


        Information seeking and integration is a complex cognitive task that consumes enormous time and effort. Inspired by the remarkable progress of Large Language Models, recent works attempt to solve this task by combining LLMs and search engines. However, these methods still obtain unsatisfying performance due to three challenges: (1) complex requests often cannot be accurately and completely retrieved by the search engine once (2) corresponding information to be integrated is spread over multiple web pages along with massive noise, and (3) a large number of web pages with long contents may quickly exceed the maximum context length of LLMs. Inspired by the cognitive process when humans solve these problems, we introduce MindSearch to mimic the human minds in web information seeking and integration, which can be instantiated by a simple yet effective LLM-based multi-agent framework. The WebPlanner models the human mind of multi-step information seeking as a dynamic graph construction process: it decomposes the user query into atomic sub-questions as nodes in the graph and progressively extends the graph based on the search result from WebSearcher. Tasked with each sub-question, WebSearcher performs hierarchical information retrieval with search engines and collects valuable information for WebPlanner. The multi-agent design of MindSearch enables the whole framework to seek and integrate information parallelly from larger-scale (e.g., more than 300) web pages in 3 minutes, which is worth 3 hours of human effort. MindSearch demonstrates significant improvement in the response quality in terms of depth and breadth, on both close-set and open-set QA problems. Besides, responses from MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web and Perplexity.ai applications, which implies that MindSearch can already deliver a competitive solution to the proprietary AI search engine.
        △ Less


Submitted 29 July, 2024; 
      originally announced July 2024.
      
    

Comments:
Technical Report. Project Page: https://mindsearch.netlify.app Code: https://github.com/InternLM/MindSearch




arXiv:2407.20181
 [pdf, other] 


cs.CR
cs.AI
cs.DC
cs.LG



      
        Blockchain for Large Language Model Security and Safety: A Holistic Survey
      
    

Authors:
Caleb Geren, 
      
      Amanda Board, 
      
      Gaby G. Dagher, 
      
      Tim Andersen, 
      
      Jun Zhuang


Abstract:
      
        …novel attacks associated with large language models, jeopardizing user data on a massive scale. Situated at a comparable crossroads in its development, and equally prolific to LLMs in its rampant growth, blockchain has emerged in recent years as a disruptive technology with the potential to redefine how we approach data handling. In particular, and due to it…
        ▽ More


        With the advent of accessible interfaces for interacting with large language models, there has been an associated explosion in both their commercial and academic interest. Consequently, there has also been an sudden burst of novel attacks associated with large language models, jeopardizing user data on a massive scale. Situated at a comparable crossroads in its development, and equally prolific to LLMs in its rampant growth, blockchain has emerged in recent years as a disruptive technology with the potential to redefine how we approach data handling. In particular, and due to its strong guarantees about data immutability and irrefutability as well as inherent data provenance assurances, blockchain has attracted significant attention as a means to better defend against the array of attacks affecting LLMs and further improve the quality of their responses. In this survey, we holistically evaluate current research on how blockchains are being used to help protect against LLM vulnerabilities, as well as analyze how they may further be used in novel applications. To better serve these ends, we introduce a taxonomy of blockchain for large language models (BC4LLM) and also develop various definitions to precisely capture the nature of different bodies of research in these areas. Moreover, throughout the paper, we present frameworks to contextualize broader research efforts, and in order to motivate the field further, we identify future research goals as well as challenges present in the blockchain for large language model (BC4LLM) space.
        △ Less


Submitted 26 July, 2024; 
      originally announced July 2024.
      
    

Comments:
Submitted to SIGKDD Explorations




arXiv:2407.20177
 [pdf, other] 


cs.LG
cs.AI
cs.CL
stat.ML



      
        AutoScale: Automatic Prediction of Compute-optimal Data Composition for Training LLMs


Authors:
Feiyang Kang, 
      
      Yifan Sun, 
      
      Bingbing Wen, 
      
      Si Chen, 
      
      Dawn Song, 
      
      Rafid Mahmood, 
      
      Ruoxi Jia


Abstract:
      
        To ensure performance on a diverse set of downstream tasks, LLMs are pretrained via data mixtures over different domains. In this work, we demonstrate that the optimal data composition for a fixed compute budget varies depending on the scale of the training data, suggesting that the common practice of empirically determining an optimal composition using smal…
        ▽ More


        To ensure performance on a diverse set of downstream tasks, LLMs are pretrained via data mixtures over different domains. In this work, we demonstrate that the optimal data composition for a fixed compute budget varies depending on the scale of the training data, suggesting that the common practice of empirically determining an optimal composition using small-scale experiments will not yield the optimal data mixtures when scaling up to the final model. To address this challenge, we propose *AutoScale*, an automated tool that finds a compute-optimal data composition for training at any desired target scale. AutoScale first determines the optimal composition at a small scale using a novel bilevel optimization framework, Direct Data Optimization (*DDO*), and then fits a predictor to estimate the optimal composition at larger scales. The predictor's design is inspired by our theoretical analysis of scaling laws related to data composition, which could be of independent interest. In empirical studies with pre-training 774M Decoder-only LMs (GPT-2 Large) on RedPajama dataset, AutoScale decreases validation perplexity at least 25% faster than any baseline with up to 38% speed up compared to without reweighting, achieving the best overall performance across downstream tasks. On pre-training Encoder-only LMs (BERT) with masked language modeling, DDO is shown to decrease loss on all domains while visibly improving average task performance on GLUE benchmark by 8.7% and on large-scale QA dataset (SQuAD) by 5.9% compared with without reweighting. AutoScale speeds up training by up to 28%. Our codes are open-sourced.
        △ Less


Submitted 29 July, 2024; 
      originally announced July 2024.
      
    



arXiv:2407.20174
 [pdf, other] 


cs.CV
cs.AI



      
        Advancing Multimodal Large Language Models in Chart Question Answering with Visualization-Referenced Instruction Tuning
      
    

Authors:
Xingchen Zeng, 
      
      Haichuan Lin, 
      
      Yilin Ye, 
      
      Wei Zeng


Abstract:
      
        …Specifically, we propose a novel data engine to effectively filter diverse and high-quality data from existing datasets and subsequently refine and augment the data using LLM-based generation techniques to better align with practical QA tasks and visual encodings. Then, to facilitate the adaptation to chart characteristics, we utilize the enriched data to t…
        ▽ More


        Emerging multimodal large language models (MLLMs) exhibit great potential for chart question answering (CQA). Recent efforts primarily focus on scaling up training datasets (i.e., charts, data tables, and question-answer (QA) pairs) through data collection and synthesis. However, our empirical study on existing MLLMs and CQA datasets reveals notable gaps. First, current data collection and synthesis focus on data volume and lack consideration of fine-grained visual encodings and QA tasks, resulting in unbalanced data distribution divergent from practical CQA scenarios. Second, existing work follows the training recipe of the base MLLMs initially designed for natural images, under-exploring the adaptation to unique chart characteristics, such as rich text elements. To fill the gap, we propose a visualization-referenced instruction tuning approach to guide the training dataset enhancement and model development. Specifically, we propose a novel data engine to effectively filter diverse and high-quality data from existing datasets and subsequently refine and augment the data using LLM-based generation techniques to better align with practical QA tasks and visual encodings. Then, to facilitate the adaptation to chart characteristics, we utilize the enriched data to train an MLLM by unfreezing the vision encoder and incorporating a mixture-of-resolution adaptation strategy for enhanced fine-grained recognition. Experimental results validate the effectiveness of our approach. Even with fewer training examples, our model consistently outperforms state-of-the-art CQA models on established benchmarks. We also contribute a dataset split as a benchmark for future research. Source codes and datasets of this paper are available at https://github.com/zengxingchen/ChartQA-MLLM.
        △ Less


Submitted 29 July, 2024; 
      originally announced July 2024.
      
    

Comments:
11 pages, 7 figures




Previous
    
Next
      


1
        


2
            


3
            


4
            


5
            

…




Search v0.5.6 released 2020-02-24  










About
Help





contact arXivClick here to contact arXiv
 Contact


subscribe to arXiv mailingsClick here to subscribe
 Subscribe




 





Copyright
Privacy Policy




Web Accessibility Assistance


arXiv Operational Status 
              Get status notifications via
              email
              or slack





 


