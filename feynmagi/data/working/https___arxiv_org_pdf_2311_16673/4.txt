TAHAKOM
Figure3: Vision-LanguageModels(VLMs)[9]
biasesandenhancethequalityofgeneratedcontent. However,theauthorsofthepreviouslymentionnedsurveysdonot
specifyhowtheseLLMscanbeusedinthefieldofcomputervision.
In[12]theauthorsprovideacriticalexaminationofevaluationmethodsforLLMs,addressingthe’what’,’where’,
and ’how’ of LLM assessment. It covers the range of tasks LLMs are tested on, including language processing,
reasoning,andapplicationsinvariousfieldslikemedicineandeducation. Thepaperalsoexplorestheenvironmentsand
benchmarksusedforevaluation,andthemethodologiesemployedtoassessperformance,highlightingbothsuccesses
and failures of LLMs. The aim is to offer a comprehensive understanding of LLM capabilities and guide future
evaluationstrategies. In[13], theauthorsprovideacomprehensiveoverviewofthesignificantstridesmadeinthe
domainofVisual-LanguagePretrainedModels(VLPMs),whichhaveemergedasapowerfulapproachforlearning
jointrepresentationsofvisualandlinguisticcontent. Thepaperbeginsbydefiningthegeneraltasksandtheoverarching
architectureofVLPMs,thendelvesintothespecificsoflanguageandvisiondataencodingmethods. Italsoexamines
thecorestructuresofmainstreamVLPMs,detailingthenuancesoftheirdesignandfunction. Essentialstrategiesfor
bothpretrainingandfine-tuningthesemodelsaresummarized,offeringaclearroadmapforhowtheyaredevelopedand
optimizedforvarioustasks. However,alimitationofthispaperisthatitdoesnotencompasstheverylatestadvancesin
NLP,particularlythenextgenerationoftransformermodels. AsthefieldofNLPisadvancingrapidly,withnewmodels
andtechniquescontinuallyemerging,anyoverviewcanquicklybecomeoutdated. Thispaper,whilethoroughinits
currentscope,maynotcapturethemostcutting-edgedevelopmentsthathaveoccurredsinceitswriting,whichcouldbe
crucialforresearcherslookingtobuilduponthemostrecentandadvancedworkinthefield.
4 LatestLLMs
Large language models, are cutting-edge AI systems that can grasp and produce human-like language. Drawing
inspiration from the intricacies of the human brain, they’re built on intricate neural networks, such as transformer
models. Bylearningfrommassiveamountsofdata,thesemodelscanunderstandcontext,makingtheirtextoutputsfeel
morenatural,whetherthey’reansweringyourquestionsorspinningastory. Inessence,alargelanguagemodelisa
sophisticatedAIbuddy,designedtochatandunderstandjustlikehumans. Whiletheyexhibitsignificantpotential,not
allarestraightforwardintheirimplementation.LLMs,evenwhenoptimallyconfigured,strugglewithtasksnecessitating
preciseadherencetoinstructions. LlamaIndex[14]providescompatibilitywithavastmajorityofLLMs. However,
itremainsambiguouswhetheraspecificLLMwilloperateefficientlyimmediatelyuponintegrationorifadditional
adjustmentsareimperative. Thesubsequenttablesendeavortoassessthepreliminaryexperienceassociatedwithdiverse
LlamaIndexfunctionalitiesacrossvariousLLMs. Theseevaluationsaimtomeasureperformanceanddeterminethe
extentofmodificationsrequiredtoensureoptimalfunctionality. Typically,commercialAPIs,suchasOpenAI,are
perceivedasmoredependable. Nonetheless,localopen-sourcemodelsareincreasinglyfavoredduetotheiradaptability
andcommitmenttotransparency. Acomparisonofvariousopen-sourceandpaidLLMsisshowninTable2andTable1.
4.1 ApplicationsofLLMs
In[15],theauthorsintroduceMiniGPT-v2,amodeldesignedtoserveasaunifiedinterfaceforvariousvision-language
taskssuchasimagedescription,visualquestionanswering,andvisualgrounding. Theprimarychallengeaddressed
istheuseofasinglemodeltoeffectivelyperformdiversevision-languagetasksusingmulti-modalinstructions. To
achievethis,theauthorsproposetheuseofuniqueidentifiersfordifferenttasksduringtraining. Theseidentifiershelp
5